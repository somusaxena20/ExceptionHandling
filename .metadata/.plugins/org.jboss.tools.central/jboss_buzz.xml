<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Modernize at scale with the new migration toolkit for applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/18/modernize-scale-new-migration-toolkit-applications" /><author><name>Yashwanth Maheshwaram</name></author><id>f8db9619-4659-4a29-819c-b34658a17249</id><updated>2022-11-18T07:00:00Z</updated><published>2022-11-18T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/products/mta/overview"&gt;migration toolkit for applications &lt;/a&gt;from Red Hat equips developers with tools to assess, prioritize, and modernize &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; applications across the hybrid cloud on &lt;a href="https://developers.redhat.com/products/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;. Version 6 of the toolkit is now generally available—and it's a major step up from the previous version, offering new capabilities to accelerate large-scale application modernization efforts. Read on to explore the new toolkit and see a demo.&lt;/p&gt; &lt;p&gt;Based on the open source &lt;a href="https://www.konveyor.io/"&gt;Konveyor&lt;/a&gt; project, the migration toolkit for applications provides insights and alignment for project leads and migration teams as they move to Red Hat OpenShift for a single application or a portfolio of applications. These insights are a huge boost for developers and organizations looking to modernize and migrate from their legacy platforms to the cloud.&lt;/p&gt; &lt;h2&gt;What's new in migration toolkit for applications&lt;/h2&gt; &lt;p&gt;Version 6 of the migration toolkit includes the following tools for your app modernization:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;New application inventory and assessment modules&lt;/strong&gt; that assist organizations in managing, classifying, and tagging their applications while assessing application suitability for deployment in containers, including flagging potential risks for migration strategies.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Full integration with source code and binary repositories&lt;/strong&gt; to automate the retrieval of applications for analysis along with proxy integration, including HTTP and HTTPS proxy configuration managed in the user interface.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Improved analysis capabilities&lt;/strong&gt; with new analysis modes, including source and dependency modes that parse repositories to gather dependencies and add them to the overall scope of the analysis. There is also a simplified user experience to configure the analysis scope, including open source libraries.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Enhanced role-based access control (RBAC)&lt;/strong&gt; powered by &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on technology&lt;/a&gt;, defining three new differentiated personas with different permissions to suit the needs of each user—administrator, architect, and migrator—including credentials management for multiple credential types.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;An administrator perspective&lt;/strong&gt; to provide tool-wide configuration management for administrators.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Included with a Red Hat OpenShift subscription, the migration toolkit for applications helps reduce complexity and risks to accelerate the modernization of your non-cloud-enabled applications. &lt;/p&gt; &lt;h2&gt;&lt;span&gt; &lt;/span&gt;Watch a demo&lt;/h2&gt; &lt;p&gt;To see version 6 of the migration toolkit in action, check out the following demo.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Supported modernization paths&lt;/h2&gt; &lt;p&gt;Wondering if Red Hat's migration toolkit for applications will support your modernization paths? Our team has compiled a list of the most commonly used migration paths, along with the most common use cases for the toolkit. See the &lt;a href="https://developers.redhat.com/products/mta/use-cases"&gt;migration toolkit for applications page&lt;/a&gt; for the full list of supported migration paths.&lt;/p&gt; &lt;h2&gt;Get started with the migration toolkit on OpenShift&lt;/h2&gt; &lt;p&gt;You can install the migration toolkit for applications on OpenShift 4.9+ with an Operator. See &lt;a href="https://developers.redhat.com/products/mta/getting-started"&gt;Get started with the migration toolkit for applications&lt;/a&gt; for instructions.&lt;/p&gt; &lt;p&gt;If you would like to try the command-line interface (CLI), there is also a &lt;a href="https://developers.redhat.com/products/mta/download"&gt;download&lt;/a&gt; option available.&lt;/p&gt; &lt;h2&gt; &lt;/h2&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/18/modernize-scale-new-migration-toolkit-applications" title="Modernize at scale with the new migration toolkit for applications"&gt;Modernize at scale with the new migration toolkit for applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yashwanth Maheshwaram</dc:creator><dc:date>2022-11-18T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 3.0.0.Alpha1 released - First iteration of our Jakarta EE 10 stream</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-3-0-0-alpha1-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-3-0-0-alpha1-released/</id><updated>2022-11-18T00:00:00Z</updated><content type="html">Last week, Max Andersen explained our plans for Quarkus 3 and Jakarta EE 10. I published yesterday a more detailed blog post explaining how we are building the Quarkus 3 stream. It is now time to announce Quarkus 3.0.0.Alpha1 which is the first iteration of our Quarkus 3 stream. A...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Benchmarking improved conntrack performance in OvS 3.0.0</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/17/benchmarking-improved-conntrack-performance-ovs-300" /><author><name>Robin Jarry</name></author><id>97deb5b4-748c-4ad1-93c3-722f5f8e2381</id><updated>2022-11-17T07:00:00Z</updated><published>2022-11-17T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.openvswitch.org/"&gt;Open vSwitch&lt;/a&gt; (OvS), an open source tool for creating virtual Layer 2 networks, relies in some use cases on connection tracking. The recent 3.0.0 release of OvS included &lt;a href="https://github.com/openvswitch/ovs/compare/cfba95158518...b159525903d1"&gt;this patch series&lt;/a&gt; to improve multithread scalability, which makes connection tracking more efficient when OvS is run on multiple CPUs. This article shows how to measure the performance of connection tracking with OvS.&lt;/p&gt; &lt;h2&gt;What is connection tracking and why is it critical?&lt;/h2&gt; &lt;p&gt;Connection tracking, or &lt;em&gt;conntrack,&lt;/em&gt; maintains an internal table of logical network connections (also called &lt;em&gt;flows&lt;/em&gt;). The table identifies all packets that make up each flow so that they can be handled consistently.&lt;/p&gt; &lt;p&gt;Conntrack is a requirement for network address translation (NAT)—in IP address masquerading, for example (described in detail in &lt;a href="https://www.rfc-editor.org/rfc/rfc3022"&gt;RFC 3022&lt;/a&gt;). Conntrack is also required for stateful firewalls, load balancers, intrusion detection and prevention systems, and deep packet inspection. More specifically, OvS conntrack rules are used to isolate different OpenStack virtual networks (aka &lt;em&gt;security groups&lt;/em&gt;).&lt;/p&gt; &lt;p&gt;Connection tracking is usually implemented by storing known connection entries in a table, indexed by a bidirectional 5-tuple consisting of a protocol, source address, destination address, source port, and destination port. Each entry also has a state as seen from the connection tracking system. The state (&lt;em&gt;new&lt;/em&gt;, &lt;em&gt;established&lt;/em&gt;, &lt;em&gt;closed&lt;/em&gt;, etc.) is updated every time a packet matching its 5-tuple is processed. If a received packet does not match any existing conntrack entry, a new one is created and inserted into the table.&lt;/p&gt; &lt;h2&gt;Performance aspects&lt;/h2&gt; &lt;p&gt;There are two aspects to consider when measuring conntrack performance.&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;em&gt;How many new connections can be handled per second?&lt;/em&gt; This question depends on the following details:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;What is the cost of looking up an existing connection entry for each received packet?&lt;/li&gt; &lt;li&gt;Can multiple threads insert and destroy conntrack entries concurrently?&lt;/li&gt; &lt;li&gt;What is the cost of creating a conntrack entry for a new connection?&lt;/li&gt; &lt;li&gt;How many packets are exchanged per connection?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;em&gt;How many concurrent connections can the system support?&lt;/em&gt; This question depends on the following details:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;What is the size of the conntrack table?&lt;/li&gt; &lt;li&gt;What is the duration of each individual connection?&lt;/li&gt; &lt;li&gt;After a connection has been closed, how long does the conntrack entry linger in the table until it is expunged to make room for new connections? What if the connection is not closed but no longer exchanges traffic (because the client or server crashed or disconnected)?&lt;/li&gt; &lt;li&gt;What happens when the conntrack table is full?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;These two aspects of performance are somewhat connected, because even a low rate of very long new connections causes the conntrack table to fill up eventually.&lt;/p&gt; &lt;p&gt;In order to properly size the connection tracking table, one needs to know the average number of new connections per second and their average duration. Testing also requires tuning the timeout values of the conntrack engine.&lt;/p&gt; &lt;h2 id="benchmarking-process"&gt;Benchmarking process&lt;/h2&gt; &lt;p&gt;To take the measurements necessary to answer the questions in the previous section, you need a way to simulate clients and servers. Such a system must specify how many clients and servers to test, how many connections per second they are creating, how long the connections are, and how much data is exchanged in each connection.&lt;/p&gt; &lt;p&gt;A few commercial traffic generators have these capabilities, more or less refined. This article describes how to carry out the simulation with &lt;a href="https://trex-tgn.cisco.com/"&gt;TRex&lt;/a&gt;—an open source traffic generator based on the &lt;a href="https://www.dpdk.org/"&gt;Data Plane Development Kit&lt;/a&gt; (DPDK).&lt;/p&gt; &lt;p&gt;TRex has multiple modes of operation. This article uses the &lt;a href="https://trex-tgn.cisco.com/trex/doc/trex_astf.html"&gt;advanced stateful&lt;/a&gt; (ASTF) mode, which allows TRex to simulate TCP and UDP endpoints. &lt;a href="https://github.com/cisco-system-traffic-generator/trex-core/blob/v2.99/scripts/cps_ndr.py"&gt;I have tailored a script using the TRex Python API&lt;/a&gt; to perform benchmarks in a manner like &lt;a href="https://www.rfc-editor.org/rfc/rfc2544.html"&gt;RFC 2544&lt;/a&gt;, but focusing on how many new connections can be created per second.&lt;/p&gt; &lt;p&gt;Basically, this script connects to a running TRex server started in ASTF mode and creates TCP and UDP connection profiles. These profiles are state machines representing clients and servers with dynamic IP addresses and ports. You can define the number of data exchanges and their sizes, add some arbitrary wait time to simulate network latency, etc. TRex takes care of translating your specifications into real traffic.&lt;/p&gt; &lt;p&gt;Here is a stripped down example, in Python, of a TCP connection profile:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;client = ASTFProgram(stream=True) server = ASTFProgram(stream=True) for _ in range(num_messages): client.send(message_size * b"x") server.recv(message_size) if server_wait &gt; 0: server.delay(server_wait * 1000) # trex wants microseconds server.send(message_size * b"y") client.recv(message_size) tcp_profile = ASTFTemplate( client_template=ASTFTCPClientTemplate( program=client, port=8080, cps=99, # base value which is changed during the binary search cont=True, ), server_template=ASTFTCPServerTemplate( program=server, assoc=ASTFAssociationRule(port=8080) ), )&lt;/code&gt;&lt;/pre&gt; &lt;h2 id="setup"&gt;Setup&lt;/h2&gt; &lt;p&gt;The device under test (DUT) runs the &lt;code&gt;ovs-vswitchd&lt;/code&gt; Open vSwitch daemon with the user-space DPDK datapath. The setup can be used to benchmark any connection-tracking device. This procedure is overly simple and does not represent an actual production workload. However, it allows you to stress the connection tracking code path without bothering about the external details.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates the relationship between the DUT and the traffic generator, which the test creates. Traffic simulating the clients travels from &lt;code&gt;port0&lt;/code&gt; to &lt;code&gt;port1&lt;/code&gt; on traffic generator through the DUT. Server traffic travels from &lt;code&gt;port1&lt;/code&gt; to &lt;code&gt;port0&lt;/code&gt; on the traffic generator. Conntrack flows are programmed on &lt;code&gt;br0&lt;/code&gt; to only allow &lt;em&gt;new&lt;/em&gt; connections to be established from &lt;code&gt;port0&lt;/code&gt; to &lt;code&gt;port1&lt;/code&gt; (from "clients" to "servers") and also allow the reply packets on &lt;em&gt;established&lt;/em&gt; connections from &lt;code&gt;port1&lt;/code&gt; to &lt;code&gt;port0&lt;/code&gt; (from "servers" to "clients") to go through.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_20.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_20.png?itok=aVJPnB5M" width="600" height="248" alt="Network topology diagram" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Network topology. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3 id="base-system"&gt;Base system&lt;/h3&gt; &lt;p&gt;Both the OvS user-space datapath and TRex use DPDK. The settings shown in this section are common to both machines.&lt;/p&gt; &lt;p&gt;DPDK requires compatible network interfaces. The example in this article runs on the last two ports of an Intel X710 PCI network interface. The following commands show the hardware in use:&lt;/p&gt; &lt;pre&gt; [root@* ~]# lscpu | grep -e "^Model name:" -e "^NUMA" -e MHz NUMA node(s): 1 Model name: Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz CPU MHz: 2700.087 NUMA node0 CPU(s): 0-23 [root@* ~]# grep ^MemTotal /proc/meminfo MemTotal: 65373528 kB [root@* ~]# lspci | grep X710 | tail -n2 18:00.2 Ethernet controller: Intel Corporation Ethernet Controller X710 for 10GbE SFP+ (rev 02) 18:00.3 Ethernet controller: Intel Corporation Ethernet Controller X710 for 10GbE SFP+ (rev 02) &lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To make things simpler, all commands in this article are executed as the &lt;code&gt;root&lt;/code&gt; user.&lt;/p&gt; &lt;p&gt;The CPUs used by TRex and OvS need to be isolated in order to minimize disturbance from the other tasks running on Linux. Therefore, the following commands isolate CPUs from the NUMA node where the PCI NIC is connected. CPUs 0 and 12 are left to Linux:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install -y tuned tuned-profiles-cpu-partitioning cat &gt; /etc/tuned/cpu-partitioning-variables.conf &lt;&lt;EOF isolated_cores=1-11,13-23 no_balance_cores=1-11,13-23 EOF tuned-adm profile cpu-partitioning &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, DPDK applications require huge pages. It is best to allocate them at boot time to ensure that they are all mapped to contiguous chunks of memory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &gt;&gt; /etc/default/grub &lt;&lt;EOF GRUB_CMDLINE_LINUX="\$GRUB_CMDLINE_LINUX intel_iommu=on iommu=pt" GRUB_CMDLINE_LINUX="\$GRUB_CMDLINE_LINUX hugepagesz=1G hugepages=32" EOF grub2-mkconfig -o /etc/grub2.cfg dnf install -y driverctl driverctl set-override 0000:18:00.2 vfio-pci driverctl set-override 0000:18:00.3 vfio-pci # reboot is required to apply isolcpus and allocate hugepages on boot systemctl reboot &lt;/code&gt;&lt;/pre&gt; &lt;h3 id="traffic-generator"&gt;TRex and the traffic generator&lt;/h3&gt; &lt;p&gt;TRex needs to be compiled from source. The following commands download and build the program:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install -y python3 git numactl-devel zlib-devel gcc-c++ gcc git clone https://github.com/cisco-system-traffic-generator/trex-core ~/trex cd ~/trex/linux_dpdk ./b configure taskset 0xffffffffff ./b build &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We use the following configuration in &lt;code&gt;/etc/trex_cfg.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;- version: 2 interfaces: - "18:00.2" - "18:00.3" rx_desc: 4096 tx_desc: 4096 port_info: - dest_mac: "04:3f:72:f2:8f:33" src_mac: "04:3f:72:f2:8f:32" - dest_mac: "04:3f:72:f2:8f:32" src_mac: "04:3f:72:f2:8f:33" c: 22 memory: mbuf_64: 30000 mbuf_128: 500000 mbuf_256: 30717 mbuf_512: 30720 mbuf_1024: 30720 mbuf_2048: 4096 platform: master_thread_id: 0 latency_thread_id: 12 dual_if: - socket: 0 threads: [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, we can start TRex:&lt;/p&gt; &lt;pre&gt; cd ~/trex/scripts ./t-rex-64 -i --astf &lt;/pre&gt; &lt;p&gt;The TRex daemon runs in the foreground. The &lt;a href="https://github.com/cisco-system-traffic-generator/trex-core/blob/v2.99/scripts/cps_ndr.py"&gt;&lt;code&gt;cps_ndr.py&lt;/code&gt;&lt;/a&gt; script connects to the daemon via the JSON-RPC API in a separate terminal.&lt;/p&gt; &lt;h3 id="device-under-test"&gt;The device under test&lt;/h3&gt; &lt;p&gt;First, let's compile and install DPDK:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install -y git meson ninja-build gcc python3-pyelftools git clone -b v21.11 https://github.com/DPDK/dpdk ~/dpdk cd ~/dpdk meson build taskset 0xffffff ninja -C ~/dpdk/build install &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then compile and install OVS. In the following console excerpt, I explicitly check out version 2.17.2. Version 3.0.0 will be recompiled before running all tests again:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install -y gcc-g++ make libtool autoconf automake git clone -b v2.17.2 https://github.com/openvswitch/ovs ~/ovs cd ~/ovs ./boot.sh PKG_CONFIG_PATH="/usr/local/lib64/pkgconfig" ./configure --with-dpdk=static taskset 0xffffff make install -j24 /usr/local/share/openvswitch/scripts/ovs-ctl start &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here I enable the DPDK user-space datapath and configure a bridge with two ports. For now, there is only one receive (RX) queue per port, and one CPU is assigned to poll them. I will increase these parameters along the way.&lt;/p&gt; &lt;p&gt;I set the conntrack table size to a relatively large value (5 million entries) to reduce the risk of it getting full during tests. Also, I configure the various timeout policies to match the traffic profiles I am about to send. These aggressive timeouts help prevent the table from getting full. The default timeout values are very conservative—they're too long to achieve high numbers of connections per second without filling the conntrack table:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:dpdk-init=true ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x4" /usr/local/share/openvswitch/scripts/ovs-ctl restart ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev ovs-vsctl add-port br0 port0 -- \ set interface port0 type=dpdk options:dpdk-devargs=0000:18:00.2 ovs-vsctl add-port br0 port1 -- \ set Interface port1 type=dpdk options:dpdk-devargs=0000:18:00.3 ovs-appctl dpctl/ct-set-maxconns 5000000 # creating an empty datapath record is required to add a zone timeout policy ovs-vsctl -- --id=@m create Datapath datapath_version=0 -- \ set Open_vSwitch . datapaths:"netdev"=@m ovs-vsctl add-zone-tp netdev zone=0 \ udp_first=1 udp_single=1 udp_multiple=30 tcp_syn_sent=1 \ tcp_syn_recv=1 tcp_fin_wait=1 tcp_time_wait=1 tcp_close=1 \ tcp_established=30 cat &gt; ~/ct-flows.txt &lt;&lt; EOF priority=1 ip ct_state=-trk actions=ct(table=0) priority=1 ip ct_state=+trk+new in_port=port0 actions=ct(commit),normal priority=1 ip ct_state=+trk+est actions=normal priority=0 actions=drop EOF &lt;/code&gt;&lt;/pre&gt; &lt;h2 id="test-procedure"&gt;Test procedure&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/cisco-system-traffic-generator/trex-core/blob/v2.99/scripts/cps_ndr.py"&gt;&lt;code&gt;cps_ndr.py&lt;/code&gt;&lt;/a&gt; script that I have written has multiple parameters to control the nature of the generated connections:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Ratio of TCP connections to UDP connections&lt;/li&gt; &lt;li&gt;Number of data messages (request + response) exchanged per connection (excluding protocol overhead)&lt;/li&gt; &lt;li&gt;Size of data messages in bytes (to emulate the TCP maximum segment size)&lt;/li&gt; &lt;li&gt;Time in milliseconds that the simulated servers wait before sending a response to a request&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In the context of this benchmark, I intentionally keep the size of data messages fixed to 20 bytes, to avoid being limited by the 10Gbit bandwidth.&lt;/p&gt; &lt;p&gt;I run two types of test: One with short-lived connections and the other with long-lived connections. Both the short-lived and long-lived connection profiles are tested against OVS versions 2.17.2 and 3.0.0. Different configurations are tested to check whether performance scales with the number of CPUs and receive queues.&lt;/p&gt; &lt;h3 id="short-lived-connections"&gt;Short-lived connections&lt;/h3&gt; &lt;p&gt;The parameters of this test consist of sending 40 data bytes per connection (1 request + 1 reply of 20 bytes each), with no wait by the server before sending the replies. These parameters stress the conntrack &lt;strong&gt;creation and destruction&lt;/strong&gt; code path.&lt;/p&gt; &lt;p&gt;An example run follows:&lt;/p&gt; &lt;pre&gt; [root@tgen scripts]# ./cps_ndr.py --sample-time 30 --max-iterations 8 \ &gt; --error-threshold 0.02 --udp-percent 1 --num-messages 1 \ &gt; --message-size 20 --server-wait 0 -m 1k -M 100k ... iteration #1: lower=1.0K current=50.5K upper=100K ▼▼▼ Flows: active 26.8K (50.1K/s) TX: 215Mb/s (345Kp/s) RX: 215Mb/s (345Kp/s) Size: ~4.5B err dropped: 1.6K pkts (1.6K/s) ~ 0.4746% ... iteration #2: lower=1.0K current=25.8K upper=50.5K ▲▲▲ Flows: active 12.9K (25.7K/s) TX: 112Mb/s (179Kp/s) RX: 112Mb/s (179Kp/s) Size: ~4.5B ... iteration #3: lower=25.8K current=38.1K upper=50.5K ▲▲▲ Flows: active 19.1K (38.1K/s) TX: 166Mb/s (266Kp/s) RX: 166Mb/s (266Kp/s) Size: ~4.5B ... iteration #4: lower=38.1K current=44.3K upper=50.5K ▼▼▼ Flows: active 22.2K (44.2K/s) TX: 192Mb/s (307Kp/s) RX: 191Mb/s (307Kp/s) Size: ~4.5B err dropped: 1.3K pkts (125/s) ~ 0.0408% ... iteration #5: lower=38.1K current=41.2K upper=44.3K ▲▲▲ Flows: active 20.7K (41.2K/s) TX: 178Mb/s (286Kp/s) RX: 178Mb/s (286Kp/s) Size: ~4.5B ... iteration #6: lower=41.2K current=42.8K upper=44.3K ▼▼▼ Flows: active 21.5K (42.6K/s) TX: 185Mb/s (296Kp/s) RX: 185Mb/s (296Kp/s) Size: ~4.5B err dropped: 994 pkts (99/s) ~ 0.0335% ... iteration #7: lower=41.2K current=42.0K upper=42.8K ▼▼▼ Flows: active 21.0K (41.8K/s) TX: 181Mb/s (290Kp/s) RX: 181Mb/s (290Kp/s) Size: ~4.5B err dropped: 877 pkts (87/s) ~ 0.0301% ... iteration #8: lower=41.2K current=41.6K upper=42.0K ▲▲▲ Flows: active 20.9K (41.4K/s) TX: 180Mb/s (289Kp/s) RX: 180Mb/s (289Kp/s) Size: ~4.5B &lt;/pre&gt; &lt;h3 id="long-lived-connections"&gt;Long-lived connections&lt;/h3&gt; &lt;p&gt;The parameters of this test consist of sending 20K data bytes per connection (500 requests + 500 replies of 20 bytes each) over 25 seconds. These parameters stress the conntrack &lt;strong&gt;lookup&lt;/strong&gt; code path.&lt;/p&gt; &lt;p&gt;An example run follows:&lt;/p&gt; &lt;pre&gt; [root@tgen scripts]# ./cps_ndr.py --sample-time 120 --max-iterations 8 \ &gt; --error-threshold 0.02 --udp-percent 1 --num-messages 500 \ &gt; --message-size 20 --server-wait 50 -m 500 -M 2k ... iteration #1: lower=500 current=1.2K upper=2.0K ▼▼▼ Flows: active 48.5K (1.2K/s) TX: 991Mb/s (1.5Mp/s) RX: 940Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 1.8M pkts (30.6K/s) ~ 2.4615% ... iteration #2: lower=500 current=875 upper=1.2K ▲▲▲ Flows: active 22.5K (871/s) TX: 871Mb/s (1.3Mp/s) RX: 871Mb/s (1.3Mp/s) Size: ~13.3B ... iteration #3: lower=875 current=1.1K upper=1.2K ▼▼▼ Flows: active 33.8K (1.1K/s) TX: 967Mb/s (1.4Mp/s) RX: 950Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 621K pkts (10.3K/s) ~ 0.7174% ... iteration #4: lower=875 current=968 upper=1.1K ▲▲▲ Flows: active 24.9K (965/s) TX: 961Mb/s (1.4Mp/s) RX: 962Mb/s (1.4Mp/s) Size: ~13.3B ... iteration #5: lower=968 current=1.0K upper=1.1K ▼▼▼ Flows: active 29.8K (1.0K/s) TX: 965Mb/s (1.4Mp/s) RX: 957Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 334K pkts (5.6K/s) ~ 0.3830% ... iteration #6: lower=968 current=992 upper=1.0K ▼▼▼ Flows: active 25.5K (989/s) TX: 964Mb/s (1.4Mp/s) RX: 964Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 460 pkts (460/s) ~ 0.0314% ... iteration #7: lower=968 current=980 upper=992 ▼▼▼ Flows: active 25.3K (977/s) TX: 962Mb/s (1.4Mp/s) RX: 962Mb/s (1.4Mp/s) Size: ~13.3B err dropped: 397 pkts (397/s) ~ 0.0272% ... iteration #8: lower=968 current=974 upper=980 ▲▲▲ Flows: active 25.1K (971/s) TX: 969Mb/s (1.5Mp/s) RX: 969Mb/s (1.5Mp/s) Size: ~13.3B &lt;/pre&gt; &lt;h2 id="results"&gt;Performance statistics&lt;/h2&gt; &lt;p&gt;This section presents results of runs with varying numbers of CPUs and queues on my test system. The numbers that I measured should be taken with a grain of salt. Connection tracking performance is highly dependent on hardware, traffic profile, and overall system load. I provide the statistics here just to give a general idea of the improvement brought by OVS 3.0.0.&lt;/p&gt; &lt;h3 id="traffic-generator-calibration"&gt;Baseline results for comparison&lt;/h3&gt; &lt;p&gt;For reference, the tests were executed with a cable connecting &lt;code&gt;port0&lt;/code&gt; and &lt;code&gt;port1&lt;/code&gt; of the traffic generator machine. This is the maximum performance TRex is able to achieve with this configuration and hardware.&lt;/p&gt; &lt;table&gt;&lt;caption&gt;Table 1: Maximum traffic generator performance.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type&lt;/th&gt; &lt;th&gt;Connection rate&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Short-lived&lt;/td&gt; &lt;td&gt;1.8M conn/s&lt;/td&gt; &lt;td&gt;1.7M&lt;/td&gt; &lt;td&gt;8.4G bit/s&lt;/td&gt; &lt;td&gt;12.7M pkt/s&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Long-lived&lt;/td&gt; &lt;td&gt;11.1K conn/s&lt;/td&gt; &lt;td&gt;898K&lt;/td&gt; &lt;td&gt;8.0G bit/s&lt;/td&gt; &lt;td&gt;11.4M pkt/s&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id="1-cpu-1-queue-per-port-without-connection-tracking"&gt;1 CPU, 1 queue per port, without connection tracking&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x4" ovs-vsctl set Interface port0 options:n_rxq=1 ovs-vsctl set Interface port1 options:n_rxq=1 ovs-ofctl del-flows br0 ovs-ofctl add-flow br0 action=normal &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 2: Short-lived connections with 1 CPU, 1 queue per port, without connection tracking.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;1.0M conn/s&lt;/td&gt; &lt;td&gt;524.8K&lt;/td&gt; &lt;td&gt;4.5G bit/s&lt;/td&gt; &lt;td&gt;7.3M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;1.0M conn/s&lt;/td&gt; &lt;td&gt;513.1K&lt;/td&gt; &lt;td&gt;4.5G bit/s&lt;/td&gt; &lt;td&gt;7.1M pkt/s&lt;/td&gt; &lt;td&gt;-1.74%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 3: Long-lived connections with 1 CPU, 1 queue per port, without connection tracking.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;3.1K conn/s&lt;/td&gt; &lt;td&gt;79.9K&lt;/td&gt; &lt;td&gt;3.1G bit/s&lt;/td&gt; &lt;td&gt;4.7M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;2.8K conn/s&lt;/td&gt; &lt;td&gt;71.9K&lt;/td&gt; &lt;td&gt;2.8G bit/s&lt;/td&gt; &lt;td&gt;4.2M pkt/s&lt;/td&gt; &lt;td&gt;-9.82%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;There is a drop in performance, without connection tracking enabled, between versions 2.17.2 and 3.0.0. This drop is completely unrelated to the conntrack optimization patch series I am focusing on. It might be caused by some discrepancies in the test procedure, but it might also have been introduced by another patch series between the two tested versions.&lt;/p&gt; &lt;h3 id="1-cpu-1-queue-per-port"&gt;1 CPU, 1 queue per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x4" ovs-vsctl set Interface port0 options:n_rxq=1 ovs-vsctl set Interface port1 options:n_rxq=1 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 4: Short-lived connections with 1 CPU, 1 queue per port, with connection tracking.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;39.7K conn/s&lt;/td&gt; &lt;td&gt;20.0K&lt;/td&gt; &lt;td&gt;172.0M bit/s&lt;/td&gt; &lt;td&gt;275.8K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;48.2K conn/s&lt;/td&gt; &lt;td&gt;24.3K&lt;/td&gt; &lt;td&gt;208.9M bit/s&lt;/td&gt; &lt;td&gt;334.9K pkt/s&lt;/td&gt; &lt;td&gt;+21.36%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 5: Long-lived connections with 1 CPU, 1 queue per port, with connection tracking.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;959 conn/s&lt;/td&gt; &lt;td&gt;24.7K&lt;/td&gt; &lt;td&gt;956.6M bit/s&lt;/td&gt; &lt;td&gt;1.4M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;1.2K conn/s&lt;/td&gt; &lt;td&gt;31.5K&lt;/td&gt; &lt;td&gt;1.2G bit/s&lt;/td&gt; &lt;td&gt;1.8M pkt/s&lt;/td&gt; &lt;td&gt;+28.15%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Already here, we can see that the patch series improves the single-threaded performance of connection tracking during the creation, destruction, and lookup code paths. Keep these results in mind when looking at improvements in multithreaded performance.&lt;/p&gt; &lt;h3 id="2-cpus-1-queue-per-port"&gt;2 CPUs, 1 queue per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x2002" ovs-vsctl set Interface port0 options:n_rxq=1 ovs-vsctl set Interface port1 options:n_rxq=1 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 6: Short-lived connections with 2 CPUs, 1 queue per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;39.9K conn/s&lt;/td&gt; &lt;td&gt;20.0K&lt;/td&gt; &lt;td&gt;172.8M bit/s&lt;/td&gt; &lt;td&gt;277.0K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;46.8K conn/s&lt;/td&gt; &lt;td&gt;23.5K&lt;/td&gt; &lt;td&gt;202.7M bit/s&lt;/td&gt; &lt;td&gt;325.0K pkt/s&lt;/td&gt; &lt;td&gt;+17.28%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 7: Long-lived connections with 2 CPUs, 1 queue per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;885 conn/s&lt;/td&gt; &lt;td&gt;22.7K&lt;/td&gt; &lt;td&gt;883.1M bit/s&lt;/td&gt; &lt;td&gt;1.3M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;1.1K conn/s&lt;/td&gt; &lt;td&gt;28.6K&lt;/td&gt; &lt;td&gt;1.1G bit/s&lt;/td&gt; &lt;td&gt;1.7M pkt/s&lt;/td&gt; &lt;td&gt;+25.19%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;It is worth noting that assigning twice as many CPUs to do packet processing does not double the performance. Far from it, in fact. The numbers are exactly the same (if not lower) than with only one CPU.&lt;/p&gt; &lt;p&gt;This surprising result might be caused because there is only one RX queue per port and each CPU processes a single port.&lt;/p&gt; &lt;h3 id="2-cpus-2-queues-per-port"&gt;2 CPUs, 2 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x2002" ovs-vsctl set Interface port0 options:n_rxq=2 ovs-vsctl set Interface port1 options:n_rxq=2 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 8: Short-lived connections with 2 CPUs, 2 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;48.3K conn/s&lt;/td&gt; &lt;td&gt;24.3K&lt;/td&gt; &lt;td&gt;208.8M bit/s&lt;/td&gt; &lt;td&gt;334.8K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;65.9K conn/s&lt;/td&gt; &lt;td&gt;33.2K&lt;/td&gt; &lt;td&gt;286.8M bit/s&lt;/td&gt; &lt;td&gt;459.9K pkt/s&lt;/td&gt; &lt;td&gt;+36.41%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;For short-lived connections, we begin to see improvement beyond the single-threaded performance gain. Lock contention was reduced during the insertion and deletion of conntrack entries.&lt;/p&gt; &lt;table&gt;&lt;caption&gt; &lt;p&gt;Table 9: Long-lived connections with 2 CPUs, 2 queues per port.&lt;/p&gt; &lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;1.1K conn/s&lt;/td&gt; &lt;td&gt;29.1K&lt;/td&gt; &lt;td&gt;1.1G bit/s&lt;/td&gt; &lt;td&gt;1.7M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;1.4K conn/s&lt;/td&gt; &lt;td&gt;37.0K&lt;/td&gt; &lt;td&gt;1.4G bit/s&lt;/td&gt; &lt;td&gt;2.2M pkt/s&lt;/td&gt; &lt;td&gt;+26.77%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;With two CPUs and two queues, if we take the single-threaded performance out of the picture, there seems to be no improvement in conntrack lookup for long-lived connections.&lt;/p&gt; &lt;h3 id="4-cpus-2-queues-per-port"&gt;4 CPUs, 2 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x6006" ovs-vsctl set Interface port0 options:n_rxq=2 ovs-vsctl set Interface port1 options:n_rxq=2 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 10: Short-lived connections with 4 CPUs, 2 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;47.4K conn/s&lt;/td&gt; &lt;td&gt;23.9K&lt;/td&gt; &lt;td&gt;206.2M bit/s&lt;/td&gt; &lt;td&gt;330.6K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;49.1K conn/s&lt;/td&gt; &lt;td&gt;24.7K&lt;/td&gt; &lt;td&gt;212.1M bit/s&lt;/td&gt; &lt;td&gt;340.1K pkt/s&lt;/td&gt; &lt;td&gt; &lt;p&gt;+3.53%&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The short-lived connection rate performance has dropped in 3.0.0. This is not a fluke: The numbers are consistent across multiple runs. This drop warrants some scrutiny, but does not invalidate all the work that has been done.&lt;/p&gt; &lt;table&gt;&lt;caption&gt;Table 11: Long-lived connections with 4 CPUs, 2 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;981 conn/s&lt;/td&gt; &lt;td&gt;25.2K&lt;/td&gt; &lt;td&gt;977.7M bit/s&lt;/td&gt; &lt;td&gt;1.5M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;2.0K conn/s&lt;/td&gt; &lt;td&gt;52.4K&lt;/td&gt; &lt;td&gt;2.0G bit/s&lt;/td&gt; &lt;td&gt;3.1M pkt/s&lt;/td&gt; &lt;td&gt;+108.31%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;With four CPUs and two queues per port, long-lived connections tracking is starting to scale up.&lt;/p&gt; &lt;h3 id="4-cpus-4-queues-per-port"&gt;4 CPUs, 4 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x6006" ovs-vsctl set Interface port0 options:n_rxq=4 ovs-vsctl set Interface port1 options:n_rxq=4 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 12: Short-lived connections with 4 CPUs, 4 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;66.1K conn/s&lt;/td&gt; &lt;td&gt;33.2K&lt;/td&gt; &lt;td&gt;286.4M bit/s&lt;/td&gt; &lt;td&gt;459.2K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;100.8K conn/s&lt;/td&gt; &lt;td&gt;50.6K&lt;/td&gt; &lt;td&gt;437.0M bit/s&lt;/td&gt; &lt;td&gt;700.6K pkt/s&lt;/td&gt; &lt;td&gt;+52.55%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 13: Long-lived connections with 4 CPUs, 4 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;996 conn/s&lt;/td&gt; &lt;td&gt;25.9K&lt;/td&gt; &lt;td&gt;994.2M bit/s&lt;/td&gt; &lt;td&gt;1.5M pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;2.6K conn/s&lt;/td&gt; &lt;td&gt;67.0K&lt;/td&gt; &lt;td&gt;2.6G bit/s&lt;/td&gt; &lt;td&gt;3.9M pkt/s&lt;/td&gt; &lt;td&gt;+162.89%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id="8-cpus-4-queues-per-port"&gt;8 CPUs, 4 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x1e01e" ovs-vsctl set Interface port0 options:n_rxq=4 ovs-vsctl set Interface port1 options:n_rxq=4 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 14: Short-lived connections with 8 CPUs, 4 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;62.2K conn/s&lt;/td&gt; &lt;td&gt;31.3K&lt;/td&gt; &lt;td&gt;269.8M bit/s&lt;/td&gt; &lt;td&gt;432.5K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;90.1K conn/s&lt;/td&gt; &lt;td&gt;45.2K&lt;/td&gt; &lt;td&gt;390.9M bit/s&lt;/td&gt; &lt;td&gt;626.7K pkt/s&lt;/td&gt; &lt;td&gt;+44.89%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 15: Long-lived connections with 8 CPUs, 4 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;576 conn/s&lt;/td&gt; &lt;td&gt;17.1K&lt;/td&gt; &lt;td&gt;567.2M bit/s&lt;/td&gt; &lt;td&gt;852.5K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;3.8K conn/s&lt;/td&gt; &lt;td&gt;97.8K&lt;/td&gt; &lt;td&gt;3.8G bit/s&lt;/td&gt; &lt;td&gt;5.7M pkt/s&lt;/td&gt; &lt;td&gt;+562.76%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id="8-cpus-8-queues-per-port"&gt;8 CPUs, 8 queues per port&lt;/h3&gt; &lt;p&gt;The results in this section were achieved with the following DUT configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ovs-vsctl set open_vswitch . other_config:pmd-cpu-mask="0x1e01e" ovs-vsctl set Interface port0 options:n_rxq=8 ovs-vsctl set Interface port1 options:n_rxq=8 ovs-ofctl del-flows br0 ovs-ofctl add-flows br0 ~/ct-flows.txt &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;caption&gt;Table 16: Short-lived connections with 8 CPUs, 8 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Short-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;50.6K conn/s&lt;/td&gt; &lt;td&gt;25.5K&lt;/td&gt; &lt;td&gt;219.5M bit/s&lt;/td&gt; &lt;td&gt;351.9K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;100.9K conn/s&lt;/td&gt; &lt;td&gt;50.7K&lt;/td&gt; &lt;td&gt;436.0M bit/s&lt;/td&gt; &lt;td&gt;698.9K pkt/s&lt;/td&gt; &lt;td&gt;+99.36%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table&gt;&lt;caption&gt;Table 17: Long-lived connections with 8 CPUs, 8 queues per port.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Version&lt;/th&gt; &lt;th&gt;Long-lived connections&lt;/th&gt; &lt;th&gt;Active flows&lt;/th&gt; &lt;th&gt;Bandwidth&lt;/th&gt; &lt;th&gt;Packet rate&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2.17.2&lt;/td&gt; &lt;td&gt;541 conn/s&lt;/td&gt; &lt;td&gt;14.0K&lt;/td&gt; &lt;td&gt;539.2M bit/s&lt;/td&gt; &lt;td&gt;810.3K pkt/s&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.0.0&lt;/td&gt; &lt;td&gt;4.8K conn/s&lt;/td&gt; &lt;td&gt;124.1K&lt;/td&gt; &lt;td&gt;4.8G bit/s&lt;/td&gt; &lt;td&gt;7.2M pkt/s&lt;/td&gt; &lt;td&gt;+792.83%&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id="analysis"&gt;Performance improvements in version 3.0.0 of Open vSwitch&lt;/h2&gt; &lt;p&gt;Using the tools in this article, I have been able to record advances made in version 3.0.0 in scaling and in handling long-lived connections.&lt;/p&gt; &lt;h3 id="scaling"&gt;Scaling&lt;/h3&gt; &lt;p&gt;Figure 2 shows how many insertions and deletions per second were achieved on different system configurations for short-lived connections.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_13.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig2_13.png?itok=x4T-r8hk" width="600" height="372" alt="Chart showing improvements in scaling of short-lived connections tracking in version 3.0.0" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Improvements in scaling of short-lived connections tracking in version 3.0.0. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Apart from the small blip with 4 CPUs and 2 queues per port, the conntrack insertion and deletion code path has improved consistently in OvS 3.0.0. The multithreaded lock contention remains, but is less noticeable than with OvS 2.17.2.&lt;/p&gt; &lt;p&gt;Figure 3 shows how many insertions and deletions per second were achieved on different system configurations for long-lived connections.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig3_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig3_6.png?itok=F-iPPYTf" width="600" height="369" alt="Chart showing improvements in scaling of long-lived connections tracking in version 3.0.0." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Improvements in scaling of long-lived connections tracking in version 3.0.0. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Long-lived connections tracking is where the optimizations done in OvS 3.0.0 really shine. The reduction in multithreaded lock contention with conntrack lookup makes the performance scale significantly better with the number of CPUs.&lt;/p&gt; &lt;h3 id="profiling"&gt;Performance during high traffic&lt;/h3&gt; &lt;p&gt;The following commands generate profiling reports using the Linux kernel's &lt;a href="http://perf.wiki.kernel.org/"&gt;perf command&lt;/a&gt;. I measured the performance of both version 2.17.2 and version 3.0.0 for 8 CPUs and 8 RX queues under a maximum load for long-lived connections, with conntrack flows enabled. Only the events of a single CPU were captured:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;perf record -g -C 1 sleep 60 perf report -U --no-children | grep '\[[\.k]\]' | head -15 &gt; profile-$version.txt &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the subsections that follow, I have manually annotated lines that are directly related to acquiring mutexes so that they start with a &lt;code&gt;*&lt;/code&gt; character. When a CPU is waiting for a mutex acquisition, it is not processing any network traffic, but waiting for another CPU to release the lock.&lt;/p&gt; &lt;h4 id="2172"&gt;Performance in version 2.17.2&lt;/h4&gt; &lt;p&gt;The profiled CPU spends almost 40% of its cycles acquiring locks and waiting for other CPUs to release locks:&lt;/p&gt; &lt;pre&gt; * 30.99% pmd-c01/id:5 libc.so.6 [.] pthread_mutex_lock@@GLIBC_2.2.5 12.27% pmd-c01/id:5 ovs-vswitchd [.] dp_netdev_process_rxq_port 5.18% pmd-c01/id:5 ovs-vswitchd [.] netdev_dpdk_rxq_recv 4.24% pmd-c01/id:5 ovs-vswitchd [.] pmd_thread_main 3.93% pmd-c01/id:5 ovs-vswitchd [.] pmd_perf_end_iteration * 3.63% pmd-c01/id:5 libc.so.6 [.] __GI___pthread_mutex_unlock_usercnt 3.62% pmd-c01/id:5 ovs-vswitchd [.] i40e_recv_pkts_vec_avx2 * 2.76% pmd-c01/id:5 [kernel.kallsyms] [k] syscall_exit_to_user_mode * 0.91% pmd-c01/id:5 libc.so.6 [.] __GI___lll_lock_wait * 0.18% pmd-c01/id:5 [kernel.kallsyms] [k] __x64_sys_futex * 0.17% pmd-c01/id:5 [kernel.kallsyms] [k] futex_wait * 0.12% pmd-c01/id:5 [kernel.kallsyms] [k] entry_SYSCALL_64_after_hwframe * 0.11% pmd-c01/id:5 libc.so.6 [.] __GI___lll_lock_wake * 0.08% pmd-c01/id:5 [kernel.kallsyms] [k] do_syscall_64 * 0.06% pmd-c01/id:5 [kernel.kallsyms] [k] do_futex&lt;/pre&gt; &lt;h4 id="300"&gt;Performance in version 3.0.0&lt;/h4&gt; &lt;p&gt;It is obvious that 3.0.0 has much less lock contention and therefore scales better with the number of CPUs:&lt;/p&gt; &lt;pre&gt; 15.30% pmd-c01/id:5 ovs-vswitchd [.] dp_netdev_input__ 8.62% pmd-c01/id:5 ovs-vswitchd [.] conn_key_lookup 7.88% pmd-c01/id:5 ovs-vswitchd [.] miniflow_extract 7.75% pmd-c01/id:5 ovs-vswitchd [.] cmap_find * 6.92% pmd-c01/id:5 libc.so.6 [.] pthread_mutex_lock@@GLIBC_2.2.5 5.15% pmd-c01/id:5 ovs-vswitchd [.] dpcls_subtable_lookup_mf_u0w4_u1w1 4.16% pmd-c01/id:5 ovs-vswitchd [.] cmap_find_batch 4.10% pmd-c01/id:5 ovs-vswitchd [.] tcp_conn_update 3.86% pmd-c01/id:5 ovs-vswitchd [.] dpcls_subtable_lookup_mf_u0w5_u1w1 3.51% pmd-c01/id:5 ovs-vswitchd [.] conntrack_execute 3.42% pmd-c01/id:5 ovs-vswitchd [.] i40e_xmit_fixed_burst_vec_avx2 0.77% pmd-c01/id:5 ovs-vswitchd [.] dp_execute_cb 0.72% pmd-c01/id:5 ovs-vswitchd [.] netdev_dpdk_rxq_recv 0.07% pmd-c01/id:5 ovs-vswitchd [.] i40e_xmit_pkts_vec_avx2 0.04% pmd-c01/id:5 ovs-vswitchd [.] dp_netdev_input &lt;/pre&gt; &lt;h2 id="final-words"&gt;Final words&lt;/h2&gt; &lt;p&gt;I hope this gave you some ideas for benchmarking and profiling connection tracking with TRex and &lt;code&gt;perf&lt;/code&gt;. Please leave any questions you have in comments on this article.&lt;/p&gt; &lt;p&gt;Kudos to Paolo Valerio and Gaëtan Rivet for their work on optimizing the user space OvS conntrack implementation.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/17/benchmarking-improved-conntrack-performance-ovs-300" title="Benchmarking improved conntrack performance in OvS 3.0.0"&gt;Benchmarking improved conntrack performance in OvS 3.0.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Robin Jarry</dc:creator><dc:date>2022-11-17T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus Newsletter #26 - November</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-newsletter-26/" /><author><name>James Cobb</name></author><id>https://quarkus.io/blog/quarkus-newsletter-26/</id><updated>2022-11-17T00:00:00Z</updated><content type="html">Yep, you guessed it, it’s time for the November Quarkus newsletter! Learn about the 2024 Observability roadmap with Roberto Cortez’s article. Read about making web applications more resilient against certain types of fraud with Erik Costlow’s "Quarkus Defends REST APIs against Attack" article. Watch a cool video to see how...</content><dc:creator>James Cobb</dc:creator></entry><entry><title>What's new in Red Hat Enterprise Linux 9.1</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/16/whats-new-red-hat-enterprise-linux-91" /><author><name>Nikhil Mungale, Alex Krikos</name></author><id>2b053bfb-3418-4d0f-a338-e2dda781d6ab</id><updated>2022-11-16T09:00:00Z</updated><published>2022-11-16T09:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; 9.1 brings new features and enhancements that deliver a more secure and consistent foundation for open, hybrid cloud environments and allow organizations to deliver workloads, applications, and services faster and more efficiently. Red Hat Enterprise Linux 9.1 can be &lt;a href="https://developers.redhat.com/content-gateway/file/rhel/9.1/rhel-baseos-9.1-x86_64-dvd.iso"&gt;downloaded&lt;/a&gt; at no cost as a part of the &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;Red Hat Developer Subscription for Individuals&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article summarizes some of the ways Red Hat Enterprise Linux 9.1 further improves the developer experience.&lt;/p&gt; &lt;h2&gt;The latest language runtimes and tools&lt;/h2&gt; &lt;p&gt;Many of the most popular languages in enterprise and web development have seen upgrades.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/topics/ruby/all"&gt;Ruby&lt;/a&gt; 3.1 is a new release that includes a new experimental in-process just-in-time (JIT) compiler. The JIT achieves both a fast warmup time and performance improvements on most real-world software, producing up to a 22% performance improvement on railsbench and 39% on liquid-render. The new debugger improves performance, including multi-thread and multi-process debugging, without slowing down applications.&lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Maven 3.8 is a new version of the build automation tool used to build, publish, deploy, and manage projects, primarily in &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;. To mitigate the risks of domain hijacking and man-in-the-middle (MITM) attacks, Maven 3.8 blocks HTTP access to external repositories, making HTTPS the default access protocol. This restriction adds an additional security layer for applications.&lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/products/dotnet/overview"&gt;.NET 7&lt;/a&gt; is the new version of .NET that brings many improvements in start-up and steady-state performance, as well as OpenTelemetry support. .NET 7, as a part of Red Hat Enterprise Linux 9.1, helps developers build cloud-native applications. With .NET 7, it is easier to modernize applications from legacy .NET versions.&lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; 18 is a new version that includes an update of the V8 JavaScript engine, a global fetch enabled by default, and a core test runner module. Node.js 18 has launched an experimental &lt;code&gt;fetch()&lt;/code&gt; API and Web Stream browser-compatible APIs for developers. Another available global API in Node.js 18 is BroadcastChannel, which is dedicated to cross-context communication between multiple tabs of the same origin.&lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/topics/php/all"&gt;PHP&lt;/a&gt; 8.1 is a major version update offering many new features, including enums, read-only properties, first-class callable syntax, fibers, intersection types, and performance improvements.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The latest versions of toolsets and compilers&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux 9.1 is built with GCC 11.3 and offers updated versions of the &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;LLVM&lt;/a&gt; (14.0.6), Rust (1.62.1), and Go (1.18.4) toolsets, enabling developers to modernize their applications with the latest compilers and toolsets.&lt;/p&gt; &lt;h3&gt;GCC&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux 9.1 comes with the new &lt;a href="https://developers.redhat.com/articles/2022/04/25/new-c-features-gcc-12"&gt;GCC Toolset version 12&lt;/a&gt;, which features the latest versions of development tools, with bug fixes and enhancements for Annobin and the GDB debugger. GCC 12 is available as an application stream in the form of a software collection in the AppStream repository and has critical bug fixes and enhancements that come from upstream GCC.&lt;/p&gt; &lt;h3&gt;LLVM&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux 9.1 comes with LLVM toolset version 14.0, which comes with notable changes for x86 and ARM architectures:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Support for the Intel AVX-512 - FP16 instruction set on the x86_64 architecture.&lt;/li&gt; &lt;li&gt;More streamlined support for the latest ARMv9 64-bit architecture than previous versions.&lt;/li&gt; &lt;li&gt;Support for &lt;code&gt;-Wdeclaration-after-statement&lt;/code&gt; with C99 and later standards, and not just C89, matching GCC's behavior. An important benefit is a support for style guides that forbid mixing declarations and code but want to move to newer C standards.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For more information on LLVM 14.0, refer to the &lt;a href="https://releases.llvm.org/14.0.0/docs/ReleaseNotes.html"&gt;LLVM 14.0.0 release notes&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Rust&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux 9.1 comes with &lt;a href="https://developers.redhat.com/topics/rust"&gt;Rust&lt;/a&gt; 1.62, which has fixed issues related to the compiler and standard library. Notable enhancements in Rust 1.62 include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Support for inline assembly on 64-bit x86 and 64-bit ARM using the &lt;code&gt;core::arch::asm!&lt;/code&gt; macro.&lt;/li&gt; &lt;li&gt;A custom futex-based implementation in &lt;code&gt;Mutex&lt;/code&gt;, &lt;code&gt;CondVar&lt;/code&gt;, and &lt;code&gt;RwLock&lt;/code&gt; instead of pthreads, with new optimizations made possible by Rust language guarantees.&lt;/li&gt; &lt;li&gt;Custom exit codes from &lt;code&gt;main&lt;/code&gt;, including user-defined types that implement the newly-stabilized &lt;code&gt;Termination&lt;/code&gt; trait.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Go&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux 9.1 comes with Go 1.18, which adds support for generic types. Go 1.18 includes bug and security fixes to the compiler and standard library. This includes changes in &lt;code&gt;go get&lt;/code&gt;, &lt;code&gt;go mod&lt;/code&gt;, &lt;code&gt;go mod vendor&lt;/code&gt;, and &lt;code&gt;go mod tidy&lt;/code&gt;, notably:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;go get&lt;/code&gt;: Does not build or install packages in module-aware mode. Now this command handles only dependencies in &lt;code&gt;go.mod&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;go mod&lt;/code&gt;: Has enhancements in &lt;code&gt;go mod download&lt;/code&gt;. It downloads source code only for modules explicitly required in the main module's &lt;code&gt;go.mod&lt;/code&gt; file.&lt;/li&gt; &lt;li&gt;&lt;code&gt;go mod tidy&lt;/code&gt;: Retains additional checksums in the &lt;code&gt;go.sum&lt;/code&gt; file for modules whose source code is needed for the imported packages.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Enhancements for system automation and standardization&lt;/h2&gt; &lt;p&gt;Automation and management capabilities in Red Hat Enterprise Linux 9.1 are provided by the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_systems_using_the_rhel_8_web_console/getting-started-with-the-rhel-8-web-console_system-management-using-the-rhel-8-web-console"&gt;Red Hat Enterprise Linux web console&lt;/a&gt; and Red Hat Enterprise Linux system roles. These capabilities help to automate manual tasks, standardize deployment, and provide ease of management in administrative activities.&lt;/p&gt; &lt;h3&gt;Red Hat Enterprise Linux web console&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux web console provides capabilities to view and manage cryptographic policies across systems, to make those systems consistent and protect them from future attacks. Users can edit firewall policies from the web console, including updating or changing port numbers and updating descriptions.&lt;/p&gt; &lt;h3&gt;Red Hat Enterprise Linux system roles&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux provides various system roles, including storage, firewall, network, and Microsoft SQL Server. Some of the benefits enabled by these system roles are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Support for Always On availability: The Microsoft SQL Server system role introduces support for Always On availability groups, enabled through a Red Hat Enterprise Linux high availability add-on. The role can configure synchronous clusters (up to five nodes in SQL Server 2019) to scale read performance and includes support for configuration-only replicas that can reduce cluster licensing costs.&lt;/li&gt; &lt;li&gt;Support for thinly-provisioned volumes: Support in the storage system role includes adding and removing disks from storage pools, and the ability to create and attach cache volumes to existing volumes.&lt;/li&gt; &lt;li&gt;Support for network configurations: The network system role now supports network configurations using the NMState API, configuring policy-based routing, and configuring IP over Infiniband connections.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Downloads and more information&lt;/h2&gt; &lt;p&gt;We suggest you check out the following sites:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/content-gateway/file/rhel/9.1/rhel-baseos-9.1-x86_64-dvd.iso"&gt;Download RHEL 9.1 at no cost&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/rhel/getting-started"&gt;Get started with RHEL 9.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/9.1_release_notes/index"&gt;RHEL 9.1 release notes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-delivers-newest-versions-red-hat-enterprise-linux-8-9"&gt;Red Hat's RHEL 9.1 corporate press release&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Try out Red Hat Enterprise Linux 9.1 to deploy applications anywhere from bare metal to edge, and manage applications on a secure platform. Get insights to continuously analyze platforms and applications to predict risk and recommend actions.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/16/whats-new-red-hat-enterprise-linux-91" title="What's new in Red Hat Enterprise Linux 9.1"&gt;What's new in Red Hat Enterprise Linux 9.1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nikhil Mungale, Alex Krikos</dc:creator><dc:date>2022-11-16T09:00:00Z</dc:date></entry><entry><title>Why OpenShift is essential for containerized applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/16/openshift-essential-containerized-applications" /><author><name>Nitin Gupta</name></author><id>a25d4e10-1e44-4cf3-a171-72afae277888</id><updated>2022-11-16T07:00:00Z</updated><published>2022-11-16T07:00:00Z</published><summary type="html">&lt;p&gt;Over the past few decades, application development has been evolving from bare metal hosting to virtualization to &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, leading to the adoption of the &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; orchestration platform. This article traces these developments and explains how &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; provides the next level of application support.&lt;/p&gt; &lt;p&gt;There has been an explosion in the modernization of application development and deployment over the past few years. Several publications such as &lt;a href="https://www.forbes.com/sites/louiscolumbus/2018/11/04/idc-top-10-predictions-for-worldwide-it-2019/?sh=1528d5887b96"&gt;Forbes&lt;/a&gt; and &lt;a href="https://www.businesswire.com/news/home/20191029005144/en/IDC-FutureScape-Outlines-the-Impact-Digital-Supremacy-Will-Have-on-Enterprise-Transformation-and-the-IT-Industry"&gt;Business Wire&lt;/a&gt; quoted IDC's prediction that between 2018 to 2023 more than 500 million logical applications will be developed, which is equal to the number of applications built over the previous 40 years. In addition, businesses expect faster changes to applications.&lt;/p&gt; &lt;h2&gt;Virtualization and the cloud&lt;/h2&gt; &lt;p&gt;A great shift has taken place during the past 20 years from physical servers to virtual machines (VMs) and cloud hosting for applications. Although some improvements to deployment models accompanied the shifts, organizations were focused primarily on exploiting the agility in the infrastructure to make operations and deployment easier and less costly.&lt;/p&gt; &lt;p&gt;Therefore, the evolution in infrastructure left challenges in achieving organizations' goal of more rapid development and deployment of applications. Some of the major challenges were:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Lead time was very high when bringing application changes from development to production.&lt;/li&gt; &lt;li&gt;Modifying an existing feature or adding a new feature could have ripple effects throughout the application code.&lt;/li&gt; &lt;li&gt;Even small changes to an application could lead to regression-related failures.&lt;/li&gt; &lt;li&gt;Deploying a change required retesting and redeploying the entire application.&lt;/li&gt; &lt;li&gt;The application life cycle spawned complex change processes when moving from development to quality assurance (QA), integration testing, user acceptance testing, and production.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The primary reasons for all these challenges were tight coupling within application and dependencies on elements of application infrastructures such as hardware, the operating system version, operating system packages, and libraries.&lt;/p&gt; &lt;h2&gt;Containers&lt;/h2&gt; &lt;p&gt;The release of &lt;a href="https://www.docker.com"&gt;Docker&lt;/a&gt; in 2013 revolutionized application development and delivery through the use of containers. The underlying technologies had been available in Unix and Linux for quite some time (usually under different names), but Docker made it easy for the first time to create containers by hiding the complexity of the underlying operating system.&lt;/p&gt; &lt;p&gt;A container packages an application binary and all its dependencies to facilitate uploading and running it in different environments. A container is generally much lighter-weight than a VM because the container leaves out much of the software infrastructure, leaving the host system to provide it.&lt;/p&gt; &lt;p&gt;A few important advantages of containers:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Focus on the application layer.&lt;/li&gt; &lt;li&gt;Decoupling the application from the underlying computing infrastructure.&lt;/li&gt; &lt;li&gt;Facilitating agile deployment and scaling.&lt;/li&gt; &lt;li&gt;Improving application portability across any kind of infrastructure—physical, virtual, or cloud.&lt;/li&gt; &lt;li&gt;Including all the dependencies of the applications: code dependencies, libraries, versions, etc.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;A modern service can now be designed as a collection of containers, packaging each container with all its dependencies in a container and shipping it to any underlying container-ready platform. The combined application is loosely coupled to the underlying infrastructure and runs seamlessly. Thus, containers help to solve a lot of challenges discussed earlier.&lt;/p&gt; &lt;h2&gt;Kubernetes and other management requirements&lt;/h2&gt; &lt;p&gt;Running a large number of containers that have to communicate closely, while terminating and reappearing rapidly, calls for a management platform that can schedule, orchestrate, and deploy these containers at scale. Kubernetes became the de facto standard for container management after Google released that open source platform in 2014.&lt;/p&gt; &lt;p&gt;Even though Kubernetes meets a lot of the requirements for deploying containers, still more support is needed by today's enterprises. A more complete enterprise platform calls for an ecosystem of components and tooling to provide the following features:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Compatibility: Components must work together, as well as with other infrastructure investments and with cloud providers.&lt;/li&gt; &lt;li&gt;Certification: The enterprise platform must be supported &amp; certified on various underlying infrastructures providers like Bare metal, Virtualization &amp; Cloud.&lt;/li&gt; &lt;li&gt;Observability: Monitoring and logging are important for troubleshooting and debugging.&lt;/li&gt; &lt;li&gt;Operational tooling: Updates &amp; Upgrades of the platform &amp; applications must be rolled out in an automated fashion.&lt;/li&gt; &lt;li&gt;Automation: All steps in the software supply chain, from development to production, should be automated through &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Developer tooling: CI/CD should incorporate the developer experience, including the integrated development environment (IDE), application testing, and application builds. Onboarding developers into a standardized environment should be fast and smooth.&lt;/li&gt; &lt;li&gt;Security: A well-integrated security plan should control access at network, application, and Kubernetes cluster levels.&lt;/li&gt; &lt;li&gt;Integrated software defined networking (SDN): This policy layer should be integrated with existing underlay networking.&lt;/li&gt; &lt;li&gt;Integrated storage and data service (SDS): The platform should offer a certified, integrated, automated system for persistent application data.&lt;/li&gt; &lt;li&gt;Fleet management: It should be possible to manage the life cycle of multiple Kubernetes clusters from a single pane of glass.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Figure 1 shows the requirements for a model enterprise Kubernetes platform on the left, organized by the categories on the right. Such a platform lets organizations develop and deploy containerized applications faster.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/needs.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/needs.png?itok=3i-0AiA9" width="600" height="408" alt="A mature Kubernetes platform has many layers of components." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A mature Kubernetes platform has many layers of components. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;In theory, you could build this platform within your own organization by integrating the multiple different open source solutions available in the containers/Kubernetes ecosystem from the &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF), or by finding cloud providers with each service. The drawback of this do-it-yourself (DIY) approach is that it makes you invest enormous resources and people in building the enterprise Kubernetes platform and managing the lifecycle (patching, security, updates, integration) of individual components, rather than focusing on modernizing and developing new cloud native applications that can differentiate you from your competitors.&lt;/p&gt; &lt;p&gt;Adopting an enterprise-ready Kubernetes solution supported by a major enterprise could be a better approach.&lt;/p&gt; &lt;h2&gt;Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;OpenShift is a highly secure, hybrid cloud Kubernetes platform supported by Red Hat. Built around full-stack automated operations and a consistent experience across all environments, OpenShift optimizes developer productivity and enables innovation without limits.&lt;/p&gt; &lt;p&gt;OpenShift includes all the capabilities listed in the previous section for an enterprise Kubernetes model. Figure 2 illustrates how OpenShift components map to development requirements.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/first.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/first.png?itok=EDBETwhy" width="600" height="362" alt="OpenShift components meet the requirements for enterprise container development." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: OpenShift components meet the requirements for enterprise container development. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Figure 3 displays the layers and key services in OpenShift.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/second.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/second.png?itok=1EpRUyaI" width="600" height="348" alt="OpenShift services rest on top of a Kubernetes environment." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: OpenShift services rest on top of a Kubernetes environment. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;OpenShift meets the demand for containerized applications&lt;/h2&gt; &lt;p&gt;OpenShift provides a consistent cloud-based experience across different infrastructure environments, helping organizations deliver their business goals by modernizing their services and applications. There is an enormous demand for the rapid development of containerized, cloud-based applications. Red Hat OpenShift is a key platform to meet this demand.&lt;/p&gt; &lt;p&gt;Check out the &lt;a href="https://developers.redhat.com/developer-sandbox/activities/learn-kubernetes-using-red-hat-developer-sandbox-openshift"&gt;Red Hat OpenShift Sandbox&lt;/a&gt; platform for developers, testers, and operations. Build, test, and deploy applications using container technology. It's a great place to get started with Kubernetes for free.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/16/openshift-essential-containerized-applications" title="Why OpenShift is essential for containerized applications"&gt;Why OpenShift is essential for containerized applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nitin Gupta</dc:creator><dc:date>2022-11-16T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.14.1.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-14-1-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-14-1-final-released/</id><updated>2022-11-16T00:00:00Z</updated><content type="html">Today, we released Quarkus 2.14.1.Final with some bugfixes and documentation improvements on top of our 2.14.0.Final release. It is a recommended upgrade for anyone already using 2.14. If you are not already using 2.14, please refer to our migration guide. GraalVM/Mandrel upgrade We don’t usually upgrade GraalVM/Mandrel in a micro...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>How to categorize C programs by behavior</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/11/15/how-categorize-c-programs-behavior" /><author><name>Martin Sebor</name></author><id>bb47bd70-8885-4eda-b25b-9f30e4c240dc</id><updated>2022-11-15T07:00:00Z</updated><published>2022-11-15T07:00:00Z</published><summary type="html">&lt;p&gt;Most of us who write &lt;a href="https://developers.redhat.com/topics/c"&gt;C and C++&lt;/a&gt; code intuitively understand why some programs might behave differently when compiled with different compilers or different compiler options, or when running under particular conditions. We're also aware of the dangers of undefined behavior and usually try to avoid it in our code. But not all of us appreciate the many useful (and often unavoidable) aspects of undefined behavior. This article explains the kinds of behavior the C standard uses to categorize programs and their subtle, sometimes unexpected, impacts on compilers, libraries, other standards, and programmers themselves.&lt;/p&gt; &lt;p&gt;Programmers are rarely familiar with the subtle nuances of undefined behavior. Nor do programmers pay as close attention to the other kinds of behavior that C programs are subject to, or when that behavior might occur. In our discussions, we often use terms like "valid code" or "correct program" without having a shared understanding of their meanings.&lt;/p&gt; &lt;p&gt;I'm sometimes surprised that not even expert programmers, including C committee members, always agree on what the terms mean. Some don't know why the basic categorizations of program behavior were introduced into C in the first place, or their purpose.&lt;/p&gt; &lt;h2&gt;The standard is a treaty&lt;/h2&gt; &lt;p&gt;A programming language standard outlines two sets of requirements: One that implementations must follow when translating programs, and another that programmers are subject to in order for implementations to fulfill their end of the bargain. In the words of the &lt;a href="http://std.dkuug.dk/jtc1/sc22/wg14/www/charter"&gt;C Charter&lt;/a&gt;: "the standard is a treaty between implementor &lt;em&gt;(sic)&lt;/em&gt; and programmer."&lt;/p&gt; &lt;p&gt;The standard details the rules and defines the outcomes when both sides play by those rules. However, it does not and cannot define what happens when either side steps outside those rules.&lt;/p&gt; &lt;p&gt;That distinction seems straightforward. Yet, for historical reasons, details of the C standard are a bit more nuanced, and reality makes them more complicated still. The next section of this article explains how the C standard categorizes programs, depending on the extent to which they follow the rules. I'll explain why these mechanisms were introduced, and what programmers should expect of programs that fall under the different categories.&lt;/p&gt; &lt;h2&gt;Categories of programs by behavior&lt;/h2&gt; &lt;p&gt;The original goal of the C standard committee in the early 1980s was to codify the set of rules in existing implementations. This standardization was possible for features in the original &lt;a href="https://github.com/auspbro/ebook-c/blob/master/The.C.Programming.Language.2Nd.Ed%20Prentice.Hall.Brian.W.Kernighan.and.Dennis.M.Ritchie..pdf"&gt;Kernighan and Ritchie C manual&lt;/a&gt; where compilers and runtime libraries all agreed on their interpretation, but became more difficult when the compilers and libraries diverged from it. Some diverged because the manual was ambiguous, others to provide extensions useful for their target environment or CPU.&lt;/p&gt; &lt;p&gt;The committee was cautious to minimize disruption, both to existing implementations and to the large body of C code out there. But the committee did not want to diminish the value of conformance. Therefore, the committee introduced the notion of &lt;em&gt;degrees of conformance.&lt;/em&gt; Although all implementations must conform to the standard, programs can choose to conform to varying degrees, and with varying guarantees for code portability.&lt;/p&gt; &lt;p&gt;To that end, the C standard recognizes several categories of programs. Each category in turn places progressively weaker requirements on implementations, and, as a result, weaker guarantees about programs' behavior and portability.&lt;/p&gt; &lt;p&gt;Before going into the details, a few words on terminology:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;An &lt;em&gt;implementation&lt;/em&gt; consists of a compiler and the standard C library. The C library rarely stands on its own but is usually incorporated into a larger library that implements some superset of C such as POSIX, or some proprietary operating system API.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;By &lt;em&gt;behavior&lt;/em&gt;, the standard means the observable output of a program: The data it writes to open streams (such as stdout and stderr), or the accesses it performs on volatile storage (both reading from it and writing to it).&lt;/p&gt; &lt;p&gt;Formally, the term &lt;em&gt;behavior&lt;/em&gt; is defined as &lt;em&gt;external appearance or action&lt;/em&gt;. It's important to understand that this doesn't include the behavior that might be observed in a debugger, such as changing values of variables while stepping through a function, or in the output of a tracing tool attached to the program by some sort of an interprocess communication mechanism.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Finally, the term &lt;em&gt;portability&lt;/em&gt; is defined in the ISO vocabulary as the &lt;em&gt;capability of a program to be executed on various types of data processing systems without converting the program to a different language and with little or no modification&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Although the term &lt;em&gt;behavior&lt;/em&gt; is frequently used to describe the effects of a single coding construct, such as a subtraction expression or a call to a library function like &lt;code&gt;strlen&lt;/code&gt;, the implications of a construct can extend to the entire program. This is an important and often misunderstood point: The consequences of a single coding construct have an impact not just on the constructs that follow but sometimes also (and perhaps unintuitively) constructs that come before it in the program source. I'll have more to say about this topic in the &lt;a href="#undefined_behavior"&gt;section on undefined behavior&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It's also important to understand that the categories described in this article exist solely for the purpose of, and within the context of, the C standard. The same program doesn't fall under one category when compiled with compiler A or running on operating system X and under a different category when compiled with B or running on Y. Conversely, the writers of A or X don't get to decide what category a class of programs might fall into when targeting A and X, while those of B and Y get to make a different choice for their implementation. The categorization of a program is completely determined by the requirements of the C standard, even though implementers may, and often do, provide stronger guarantees about program behavior.&lt;/p&gt; &lt;p&gt;The same is true for other specifications that incorporate C, such as POSIX. The difference with the other specifications is that by doing so, implementers as noted do not change the categorization of C programs, while the authors of other specifications tend to "override" it for their own purposes. As a result, a program that falls into one category in C might fall into a different category under POSIX—a category with stronger guarantees than the C category. (The converse isn't possible without conflicting with it.)&lt;/p&gt; &lt;h3 id="well_defined_behavior"&gt;Well-defined behavior&lt;/h3&gt; &lt;p&gt;Ideally, a program is portable without change, not just to the environments intended for it, but to all possible environments, whether or not they exist in the wild, ever did, or might in the future. Given the same input, a portable program runs with the same output everywhere it's compiled today, and will continue to do so in the future. Another way we can describe a portable program like this is to call it &lt;em&gt;well-defined.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Because the fundamental purpose of a standard is to foster portability, the prime goal of the C standard is to describe the behavior of portable programs. The behavior of all other categories is a secondary concern.&lt;/p&gt; &lt;p&gt;Portable programs provide the strongest guarantees to their users. To ensure these guarantees, portable programs must:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Consist only of code with well-defined syntax and rely only on constructs with well-defined semantics.&lt;/li&gt; &lt;li&gt;Adhere to all the most stringent requirements of the C standard, and therefore: &lt;ul&gt;&lt;li&gt;Either make no unconditional uses of any &lt;a href="#optional_features"&gt;optional features&lt;/a&gt; specified by the standard, or guard the use of each feature by the appropriate &lt;code&gt;__STDC_&lt;em&gt;XXX&lt;/em&gt;__&lt;/code&gt; feature test macro.&lt;/li&gt; &lt;li&gt;Use no &lt;a href="#implementation_extensions"&gt;implementation extensions&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Stay within all minimum implementation limits specified by the standard.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Unspecified behavior&lt;/h3&gt; &lt;p&gt;The standard describes a few dozen coding constructs that might result in more than one kind of behavior (Annex J.1 lists 63 distinct cases). A couple of examples are whether the same string literals have distinct addresses, or the order in which subexpressions are evaluated. Uses of these constructs are valid code that implementations might handle in different ways that can even change from one instance to another. Because the identified constructs are valid code, their handling doesn't extend to issuing an error, either during compilation or at runtime.&lt;/p&gt; &lt;p&gt;A program with unspecified behavior is a program that is correct according to the standard, but that contains one or more coding constructs with unspecified behavior. Such a program must run successfully on every implementation (i.e., it must not crash—or &lt;em&gt;trap,&lt;/em&gt; in the parlance of the standard—or otherwise misbehave), but could have different output from one execution to another.&lt;/p&gt; &lt;p&gt;For example, the behavior (specifically the result) of the following program is unspecified. It might return either 0 or 1, depending on whether or not the compiler merges the two empty strings into a single instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;int main (void) { const char *s = ""; return s == ""; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In theory, the program could also return 0 the first time it runs and 1 the second time, although in practice that's very unlikely.&lt;/p&gt; &lt;p&gt;Not every program that contains a coding construct with unspecified behavior necessarily has unspecified behavior itself. Programs have unspecified behavior only when their output depends in an observable way on the unspecified effects of the construct.&lt;/p&gt; &lt;h3&gt;Implementation-defined behavior&lt;/h3&gt; &lt;p&gt;Besides constructs with undefined behavior, the standard also describes a number of others that might result in more than one observable behavior, but where implementations are required to document how each instance is handled. Annex J.3 contains an exhaustive list of the 127 instances of implementation-defined behavior in C.&lt;/p&gt; &lt;p&gt;It should be emphasized that, just like the programs with unspecified behavior that we discussed in the previous section, a program with implementation-defined behavior is a valid program that must run successfully. It must not crash or otherwise misbehave.&lt;/p&gt; &lt;p&gt;This category is less useful than it seems. First, simply because a construct's behavior is documented doesn't make the construct portable, so relying on it doesn't improve the portability of a program. And second, not all implementations follow the requirement to document their choices.&lt;/p&gt; &lt;h3 id="locale_defined_behavior"&gt;Locale-specific behavior&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Locale-specific behavior&lt;/em&gt; is behavior that arises when a program runs in a locale other than the default C locale. An example locale is &lt;code&gt;fr_CA&lt;/code&gt; for Canadian French. This behavior is a special category that C programs opt into, either by being translated in a specific locale, or at runtime by calling the &lt;code&gt;setlocale&lt;/code&gt; function with a first argument other than &lt;code&gt;"C"&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Implementations are expected to document the behavior of programs in specific locales. We will not consider this category further; it's effectively a special case of well-defined behavior where the output of a program can change from one environment (locale) to another.&lt;/p&gt; &lt;h3 id="undefined_behavior"&gt;Undefined behavior&lt;/h3&gt; &lt;p&gt;Programs that don't fit in any of the categories discussed so far are &lt;em&gt;undefined.&lt;/em&gt; Annex J.2 - Undefined behavior lists over 200 instances of such behavior. However, unlike the lists in the appendices of unspecified and implementation-defined behavior, Annex J.2's list is not exhaustive. This is sometimes a source of misunderstanding that is worth clearing up. Annex J.2 enumerates &lt;em&gt;only&lt;/em&gt; the instances explicitly called out in the main body of the text. Beyond those instances where the C standard explicitly makes the behavior of a construct undefined, there are many others where the standard doesn't specify the behavior under some conditions. These instances use the words &lt;em&gt;shall&lt;/em&gt; or &lt;em&gt;shall not&lt;/em&gt; to outline a requirement on programs, and don't typically appear in the annex.&lt;/p&gt; &lt;p&gt;When interpreting the standard, it's important to be aware of this caveat. Don't assume that a programming construct always works the way it's working on your current implementation, just because the construct isn't explicitly listed as undefined.&lt;/p&gt; &lt;p&gt;The C standard describes programs that contain an undefined construct as either &lt;em&gt;nonportable&lt;/em&gt; or &lt;em&gt;erroneous&lt;/em&gt;. The practical difference is that nonportable programs run correctly (don't crash or misbehave), perhaps even with the expected output, in some environments. In contrast, erroneous programs might (although they don't need to) behave erratically or crash.&lt;/p&gt; &lt;p&gt;Most real-world programs fall into the undefined category: They contain some code whose behavior is not defined by the C standard. This isn't necessarily a bad thing. The behavior may be defined by some other standard or provided as an extension of the implementation targeted by the program's authors.&lt;/p&gt; &lt;p&gt;However, these assurances don't change the categorization of the program as undefined under the rules of the C standard. The behavior may also seem to be defined by a given implementation, in the sense that the construct and the program behave as the programmer intended. But unless the implementation documents as an extension the behavior of the construct under the conditions the standard doesn't specify, it should be considered erroneous.&lt;/p&gt; &lt;p&gt;In practice, not all implementations document all their extensions. Such practices create a fertile source of subtle portability bugs, as working programs start to misbehave after a trivial upgrade of the compiler or libraries.&lt;/p&gt; &lt;p&gt;Undefined behavior can be broken down into two kinds: &lt;em&gt;compile-time&lt;/em&gt; undefined behavior and &lt;em&gt;runtime&lt;/em&gt; undefined behavior. We'll consider each category in turn.&lt;/p&gt; &lt;h4&gt;Compile-time undefined behavior&lt;/h4&gt; &lt;p&gt;&lt;em&gt;Compile-time undefined behavior&lt;/em&gt; refers to the behavior of constructs that are processed during translation, such as preprocessing directives or the evaluation of constant expressions. An example of nonportable compile-time undefined behavior is the use of an implementation-specific literal, such as a &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-12.1.0/gcc/Binary-constants.html"&gt;binary constant&lt;/a&gt; like &lt;code&gt;0b101010&lt;/code&gt; with GCC (prior to C23). An example of erroneous compile-time undefined behavior (in popular compilers like Clang and GCC) is the following definition:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;enum { e, f, g = f / e };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The semantics of the division expression are undefined when the second operand is zero. Because the compiler evaluates this division expression to process the definition of the enumeration, the problem occurs at compile time.&lt;/p&gt; &lt;p&gt;High-quality implementations diagnose erroneous compile-time undefined behavior by issuing an error message and rejecting the program. Other implementations might evaluate the division to zero, possibly as a deliberate extension, or as an incidental outcome of the coding path taken in the compiler. The example on &lt;a href="https://godbolt.org/z/oW6coq5fv"&gt;Godbolt's Compiler Explorer&lt;/a&gt; illustrates these two alternatives in popular compilers.&lt;/p&gt; &lt;p&gt;But the possibilities don't end there. Low-quality implementations might silently accept the code but emit a program that behaves erratically at runtime: It might abort or produce inaccurate output. And in other implementations, the construct might even cause the compiler itself to crash.&lt;/p&gt; &lt;p&gt;In summary, a program that contains a coding construct with compile-time undefined behavior is undefined regardless of whether the program depends on that construct in any way. And the ill effects extend beyond the program to the implementation itself. With such a compiler, conspiracy theorists would be vindicated for believing that even just compiling an undefined program might wipe out one's hard drive.&lt;/p&gt; &lt;h4&gt;Runtime undefined behavior&lt;/h4&gt; &lt;p&gt;&lt;em&gt;Runtime undefined behavior&lt;/em&gt;, on the other hand, refers to the outcome of a construct during program execution. An example of nonportable runtime undefined behavior that's pervasive in almost all programs is calling a library function that's defined neither in the program nor by the C standard, such POSIX &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/popen.html"&gt;popen&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Another example of runtime undefined behavior is calling the C standard &lt;code&gt;printf&lt;/code&gt; function with a format directive whose behavior is undefined in the C standard but defined by some other standard such as POSIX. An example is &lt;code&gt;%2$d:%1$d&lt;/code&gt;, used by POSIX to reorder the subsequent arguments in the output. Similar examples involve calling nonstandard functions that are either provided as intrinsics by the compiler or defined by the target system.&lt;/p&gt; &lt;p&gt;The most important difference from compile-time undefined behavior is that a program that contains a coding construct with runtime undefined behavior is itself undefined only if the construct is reached during the program's execution. However, if the construct is reached (or can be proven to be reached during program analysis), the effects of undefined runtime behavior can manifest at any point in the execution of the program. The effects can appear even before execution reaches the construct.&lt;/p&gt; &lt;p&gt;The pervasive reach of runtime constructs with undefined behavior is quite unintuitive, so much so that it has been colloquially referred to as &lt;em&gt;undefined behavior time travel&lt;/em&gt;. The basic reason for this phenomenon is that the standard deliberately allows implementations to execute code out of order, including in parallel. Therefore, statements with no observable side effects that don't depend on each other can be reordered, or even merged with others, for greater efficiency.&lt;/p&gt; &lt;p&gt;Another important difference from compile-time undefined behavior is the ways high-quality implementations respond to an erroneous instance when they detect it: Although they can issue a warning message (many do for a small subset of undefined constructs), the standard prohibits conforming compilers from rejecting the program unless they can prove that &lt;em&gt;every&lt;/em&gt; execution of the program is undefined.&lt;/p&gt; &lt;p&gt;In typical scenarios, this restriction virtually rules out such a response for separately compiled programs that consist of multiple translation units. Although it's common to promote warnings to errors through options like &lt;code&gt;-Werror&lt;/code&gt;, doing so must be left at the discretion of the user, because it goes against the requirement of the standard and thus renders the implementation not conforming.&lt;/p&gt; &lt;h2 id="optional_features"&gt;Optional features&lt;/h2&gt; &lt;p&gt;Starting with C99, the standard has introduced features that conforming implementations are not required to provide. In most cases, for better portability, the presence of the support can be tested with a preprocessor conditional.&lt;/p&gt; &lt;p&gt;For example, support for variably modified types, an optional feature, can be checked by testing the value of the &lt;code&gt;__STDC_NO_VLA__&lt;/code&gt; macro. The value 1 means that the feature is supported, and any other value means it's not.&lt;/p&gt; &lt;p&gt;Relying on an optional feature is well-defined, but doing so without an equivalent backup (obviously) degrades the portability of the program to implementations that do not support the feature.&lt;/p&gt; &lt;h2 id="implementation_extensions"&gt;Implementation extensions&lt;/h2&gt; &lt;p&gt;Virtually all implementations of C and C++ provide features that are not specified by the C standard at all. Most hosted environments don't implement the C standard alone, but also some superset of it, such as POSIX or some proprietary layer. Similarly, most compilers support other specifications that are supersets of C or extend C's capabilities, such as OpenMP. Many implementations also provide additional functions and capabilities that are not specified by any standard.&lt;/p&gt; &lt;p&gt;Extensions do not affect the conformance of the implementation, as long as they have no impact on the behavior of strictly conforming programs (discussed next). Implementations are required to document their extensions.&lt;/p&gt; &lt;h2&gt;Strictly conforming and conforming programs&lt;/h2&gt; &lt;p&gt;Programs whose behavior depends exclusively on portable code are termed &lt;em&gt;strictly conforming&lt;/em&gt; in the C standard. As discussed in the &lt;a href="#well_defined_behavior"&gt;section on well-defined behavior&lt;/a&gt;, such programs must not rely on any unspecified or implementation-defined constructs, use any implementation extensions, or use optional C features without a corresponding preprocessor guard. Strictly conforming programs can, however, invoke &lt;a href="#locale_defined_behavior"&gt;locale-specific behavior&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Other programs that are accepted (the standard uses the word &lt;em&gt;acceptable&lt;/em&gt;) by at least one conforming implementation are simply called &lt;em&gt;conforming&lt;/em&gt;. Conforming programs might contain code with unspecified, implementation-defined, and even undefined behavior, including syntactic extensions such as lexical tokens and additional keywords.&lt;/p&gt; &lt;p&gt;It's worth taking a minute to clarify what the standard means by acceptable: It means not only that an implementation successfully compiles the program (with or without warnings), but also that the program runs successfully to completion, and that it doesn't do so simply by accident. If it contains undefined code, it must be only nonportable code whose semantics are documented by the implementation, not code with erroneous undefined behavior (i.e., code whose semantics are not documented, even if they happen to be benign and perfectly reasonable).&lt;/p&gt; &lt;p&gt;It's important to emphasize that, although strictly conforming programs must not contain code with undefined behavior, they can contain constructs with unspecified or implementation-defined behavior. However, those that do so must avoid relying on such behavior.&lt;/p&gt; &lt;p&gt;This distinction might seem like playing word games, but it's valid and important. Remember, even the most portable programs usually contain unspecified behavior. Here's an example that's not unusual in production code:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;int main (void) { int i = 0, j = 0; return (++i - ++j); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The order in which the two subexpressions in (&lt;code&gt;++i&lt;/code&gt; and &lt;code&gt;++j&lt;/code&gt;) are evaluated is unspecified. But because they are both evaluated before the subtraction, the result of the program is zero regardless. Thus, this program doesn't depend on the unspecified behavior, and so the program is strictly conforming: It's portable to and will run with the same output in all hosted environments.&lt;/p&gt; &lt;p&gt;The same argument would apply if the operand of the return statement were just &lt;code&gt;1 - 1&lt;/code&gt;. Even constant operands are evaluated, and the order of their evaluation is also unspecified.&lt;/p&gt; &lt;h2&gt;Practical implications&lt;/h2&gt; &lt;p&gt;So what are the practical implications of all this, that we as programmers (as opposed to the C committee, or compliance engineers) might care about?&lt;/p&gt; &lt;p&gt;The categories of behavior establish a common framework for reasoning about program &lt;em&gt;correctness&lt;/em&gt;. Our number-one priority is to write correct programs, so it's rather important to have a solid, shared definition of what &lt;em&gt;correct&lt;/em&gt; means.&lt;/p&gt; &lt;p&gt;Setting aside design requirements (those are unavoidably outside the scope of this discussion) and focusing strictly on coding, a &lt;em&gt;correct program&lt;/em&gt; is a &lt;em&gt;conforming program&lt;/em&gt; that's &lt;em&gt;acceptable&lt;/em&gt; to the implementations we target.&lt;/p&gt; &lt;p&gt;With that definition in mind, deciding whether a program is correct should be easy as long as the implementations we target are conforming (including the requirement to document extensions). If all code in our program is defined either by the C standard (including unspecified behavior), or in the manuals that come with the compiler and libraries we depend on, it's correct.&lt;/p&gt; &lt;p&gt;In contrast, if our program contains code whose semantics aren't defined anywhere (i.e., nowhere in the manuals for our implementation or the rest of the system), it is, in the parlance of the C standard, erroneous. In common speech, it's buggy, and it doesn't matter whether the bug manifests itself in an observable way or is latent.&lt;/p&gt; &lt;p&gt;A corollary of this definition is that using an implementation that doesn't document some of the implementation-defined behavior or some of its extensions prevents us from relying on those features if we want correct programs. If using them cannot be avoided, we either need to accept that our programs are erroneous, or we need to redefine what correctness means to us. The latter course of action obviously presents problems when discussing our expectations with others—notably implementers, who rely on the standard definition of correctness.&lt;/p&gt; &lt;h2&gt;References&lt;/h2&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.iso.org/standard/74528.html"&gt;ISO/IEC 9899:2018 Information technology — Programming languages — C&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf"&gt;Rationale for International Standard — Programming Languages — C, Revision 5.10&lt;/a&gt;, April 2003&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2611.htm"&gt;Programming Language C — C23 Charter&lt;/a&gt;, David Keaton&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.iso.org/standard/63598.html"&gt;ISO/IEC 2382:2015 Information technology — Vocabulary&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://www.knosof.co.uk/cbook/cbook.html"&gt;The New C Standard — An Economic and Cultural Commentary&lt;/a&gt;, Derek M. Jones&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/11/15/how-categorize-c-programs-behavior" title="How to categorize C programs by behavior"&gt;How to categorize C programs by behavior&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Martin Sebor</dc:creator><dc:date>2022-11-15T07:00:00Z</dc:date></entry><entry><title type="html">gRPC made easy with Quarkus</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/grpc-made-easy-with-quarkus/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/grpc-made-easy-with-quarkus/</id><updated>2022-11-14T19:06:50Z</updated><content type="html">This article discusses how to create applications with the gRPC framework and Quarkus. We will reuse the sample Service definition from first Java gRPC application and run it as Quarkus REST application. Defining the gRPC Service Firstly, we recommend reading this article for an introduction to the gRPC framework: Getting started with gRPC on Java ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">KOGITO 1.30.0 RELEASED!</title><link rel="alternate" href="https://blog.kie.org/2022/11/kogito-1-30-0-released.html" /><author><name>Tiago Dolphine</name></author><id>https://blog.kie.org/2022/11/kogito-1-30-0-released.html</id><updated>2022-11-14T11:10:16Z</updated><content type="html">We are glad to announce that the Kogito 1.30.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Event State timeout * Allow specifying different event marshallers per channel * Start event is not behaving as expected when exclusive is false * New Avro marshallers/unmarshallers addon * Build SWF examples images with Jib * Group all Jobs service JDBC flavors on one container image * Verify that Kaniko is able to push on Kubernetes internal registry * Deprecate Github Showcase Example * Serverless Operator: Setup Nightly pipeline * Update quarkus-openapi-generator to 0.12.0 For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found. * Kogito images are available on. * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.25.0 artifacts are available at the. A detailed changelog for 1.30.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Tiago Dolphine</dc:creator></entry></feed>
